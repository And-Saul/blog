[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/canada-births-time-series/index.html",
    "href": "posts/canada-births-time-series/index.html",
    "title": "Canadian Births Time Series Analysis",
    "section": "",
    "text": "Show the code\nbirths_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/canada_births_1991_2022.csv')"
  },
  {
    "objectID": "posts/canada-births-time-series/index.html#introduction",
    "href": "posts/canada-births-time-series/index.html#introduction",
    "title": "Canadian Births Time Series Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis work was inspired from the Tidy Tuesday session titled “Canadian NHL Player Birth Dates”. I utilised the R package fpp3 to analyse the data.\n\nShould births or birth rates be investigated?\nThe choice of variable is dependent upon the question being asked. A search using ChatGPT on this question revealed the following:\nLive births are the measure of absolute organic (excluding migration) growth per measurement period that contribute to the population size. This value is utilised for economic and resource planning, such as determining demand for resources like healthcare, education, housing, and social services.\nLive birth rate is the number of live births per 1,000 individuals in the population annually. It can be used for comparisons between countries or regions, regardless of population size. Birth rates are insightful for long-term analysis, especially in aging societies or declining populations\nFor the purposes of this blog I investigated births only\n\n\nGoals of blog\nThe goals of this blog were two-fold. Primarily I was interested in developing code to reveal birth data insights. Secondly, I was interested in revealing components of the time series."
  },
  {
    "objectID": "posts/canada-births-time-series/index.html#birth-data",
    "href": "posts/canada-births-time-series/index.html#birth-data",
    "title": "Canadian Births Time Series Analysis",
    "section": "Birth data",
    "text": "Birth data\nIn order to examine time-series data with the fpp3 package, the tibble data needs to be converted into a tsibble object..\n\n\nShow the code\n# creation of tsibble object\nts_births &lt;- \nbirths_df %&gt;% \n  # create new column as year-month character - then convert it to a mth type using yearmonth function\n  mutate(ymdate = yearmonth(paste(year, month))) %&gt;% \n  as_tsibble(index = ymdate)\n\n\nA plot of the number of monthly births in Canada from January 1991 is displayed in Figure 1.\n\n\nShow the code\nts_births %&gt;% \n  autoplot(births)+\n  labs(x = \"Year/Month\",\n       y = \"Births\",\n       title = \"Canadian Registered Births\")\n\n\n\n\n\n\n\n\nFigure 1: Canadian births\n\n\n\n\n\n\nSeasonality\nWe can investigate the seasonality of the plot using the ggseason function\n\n\nShow the code\nts_births %&gt;% \n  gg_season(births, labels = \"both\")\n\n\n\n\n\n\n\n\nFigure 2: Seasonality separated into years\n\n\n\n\n\nIn Figure 2, each year is plotted separately. However, with over 30 lines this plot is difficult to interpret. Instead, seasonality can be plotted by facetting the ggplot into months.\n\n\nShow the code\nts_births %&gt;% \n  gg_subseries(births, labels = \"both\")\n\n\n\n\n\n\n\n\nFigure 3: Seasonality: trends observed for each month\n\n\n\n\n\nIn Figure 3 it appears that most births occur in July (summer) and the fewest births occur in February (winter). One can also see from this plot that births peaked at the beginning of the time series and decreased to a minimum around 2001. From 2007 until 2021 the birth rate stabilised at a relatively high level, but fell once again for the final recorded year of 2022."
  },
  {
    "objectID": "posts/canada-births-time-series/index.html#correlations",
    "href": "posts/canada-births-time-series/index.html#correlations",
    "title": "Canadian Births Time Series Analysis",
    "section": "Correlations",
    "text": "Correlations\nAn assumption of time series modelling is that the previous time point(s) influence the current time point.\n\n\nShow the code\nts_births %&gt;% \n  gg_lag(births, lag = 1:12)+\n  labs(x = \"lag(birth, y_t)\", y = \"lag(birth, y_t-k\")\n\n\n\n\n\n\n\n\nFigure 4: Lag in months\n\n\n\n\n\nFigure 4 represents the correlation between time points separated by months, depicted by the lag number. We can see that there is a strong, maximum correlation between time points separated by 12 months. This indicates yearly seasonal variation.\n\nAutocorrelation Function (ACF)\n\n\nShow the code\nts_births %&gt;% \n  ACF(births) %&gt;% \n  autoplot() +\n  labs(title = \"Canadian monthly birth data\")\n\n\n\n\n\n\n\n\nFigure 5: Correlogram\n\n\n\n\n\nThe ACF depicts the relationships we see in the lag plots. A slow decline in ACF values vs lag number indicates that the value from the current time point is substantially influenced by values of time points both close and distant. A repeated pattern of increased ACF values indicate a seasonal component in the series. In Figure 5 both trend and seasonality are present. The repeated pattern in the ACF indicates a large seasonal component. As this repeated pattern peaks at 12 and 24 months, the seasonal component is yearly. The gradual reduction in ACF value is due to the trend component."
  },
  {
    "objectID": "posts/canada-births-time-series/index.html#time-series-decompostion",
    "href": "posts/canada-births-time-series/index.html#time-series-decompostion",
    "title": "Canadian Births Time Series Analysis",
    "section": "Time Series Decompostion",
    "text": "Time Series Decompostion\n\nTransformations\nWhen viewing Figure 1, the amount of variation should be consistant. For instance, the seasonal variation amplitude may increase by a constant factor over time. In order to maintain consistent variation, a transformation may be required.\n\n\nShow the code\nlambda &lt;- \n  ts_births %&gt;% \n  features(births,features = \"guerrero\") %&gt;% \n  pull(lambda_guerrero)\n\nts_births_bc &lt;- \n  ts_births %&gt;% \n  mutate(BC_births = box_cox(births, lambda))\n\nts_births_bc %&gt;% \n  autoplot(BC_births)+\n    labs(y = \"\",\n       title = (paste0(\n         \"Transformed gas production with \\\\lambda = \",\n         round(lambda,2))))\n\n\n\n\n\n\n\n\nFigure 6: Transformed Canadian birth time trend\n\n\n\n\n\nFor the population data, a box-cox transformation value was calculated to be 0.1273038. However, the variation seen in ?@fig-fig-transformation appeared similar to the non-transformed data in Figure 1, so therefore data transformation was not implemented for further analysis.\n\n\nARIMA\nARIMA models aim to describe the autocorrelations in the data\n\nStage 1\nInvestigate the differencing between data points. The code below plots the difference between successive time points ie \\(y_t\\) and \\(y_{t-1}\\).\n\n\nShow the code\nts_births %&gt;% \n  mutate(differencing = difference(births, lag=12)) %&gt;% \n   autoplot(differencing) + labs(subtitle = \"Changes in monthly births\")\n\n\n\n\n\n\n\n\nFigure 7: Differencing by one month\n\n\n\n\n\nIn figure Figure 7 the differencing by one month has not created a non-stationary plot. However, when differencing by 12 months (Figure 8) less of the seasonality is present.\n\n\nShow the code\nts_births %&gt;% \n  mutate(differencing = difference(births, lag=12)) %&gt;% \n   autoplot(differencing) + labs(subtitle = \"Changes in monthly births\")\n\n\n\n\n\n\n\n\nFigure 8: Differencing by twelve months\n\n\n\n\n\n\n\nShow the code\nts_births %&gt;% \n  ACF(difference(births, lag=12)) %&gt;% \n  autoplot() + labs(subtitle = \"Changes in monthly births\")\n\n\n\n\n\n\n\n\nFigure 9: ACF plot of twelve month differencing\n\n\n\n\n\nFigure Figure 9 demonstrates the non-stationary differencing effect in an ACF plot. Only lags around 12 were not greater than the significance levels (blue lines). The slow decay of lagged values indicate that previous values heavily influence the current value. Only those values around 12 months have little to no influence.\n\n\nShow the code\nts_births %&gt;% \n  gg_tsdisplay(difference(births), plot_type='partial')\n\n\n\n\n\n\n\n\n\n\n\n\nSTL decomposition (Seasonal and Trend decomposition using LOESS)\nSTL decomposition involves splitting up the data into trend/cyclical, seasonal and residual components. If it has been ascertained that the decomposition is multiplicative then components will need to be transformed. The Canadian population data appears additive and no transformation is deemed necessary.\n\n\nShow the code\n dcmp &lt;- \n  ts_births_bc %&gt;% \n  model(stl = STL(births))\n\n\n\n\nShow the code\ncomponents(dcmp) |&gt;\n  as_tsibble() |&gt;\n  autoplot(births, colour=\"gray\") +\n  geom_line(aes(y=trend), colour = \"#D55E00\") +\n  labs(\n    y = \"Births\",\n    title = \"Canadian Birth Data\"\n  )\n\n\n\n\n\n\n\n\nFigure 10: Trend pattern overlaying the data\n\n\n\n\n\nFigure 10 demonstrates the trend component overlaying the complete plot.\n\n\nShow the code\n ts_births_bc %&gt;% \n  model(stl = STL(births, robust = F)) %&gt;% \n  components() %&gt;% autoplot()\n\n\n\n\n\n\n\n\nFigure 11: STL decomposition\n\n\n\n\n\nFigure 11 is a representation of the plot divided into the three STL components. The trend component is maximum at the beginning of the trace, then decreases to its minimum, finally regaining most of its gains with a period of stability before decreasing during the covid period. It is noteworthy that the seasonal component can change slowly over time. The bars on the side of each plots have the same length."
  },
  {
    "objectID": "posts/canada-births-time-series/index.html#forecasting",
    "href": "posts/canada-births-time-series/index.html#forecasting",
    "title": "Canadian Births Time Series Analysis",
    "section": "Forecasting",
    "text": "Forecasting\nBaseline (simple) forecasting methods include the mean, naive and seasonal naive methods. These methods often act as benchmarks to more complicated techniques\n\n\nShow the code\n# set training data before 2018\ntrain &lt;- \n  ts_births %&gt;% \n  filter(year &lt;2018)\n\n#set period for forecast data\npred_pop &lt;- \n  ts_births %&gt;% \n  filter(year &gt;=2018)\n\n#fit data\npop_fit &lt;- \n  train %&gt;% \n  model(\n    Mean = MEAN(births),\n    `Naïve` = NAIVE(births),\n    `Seasonal naïve` = SNAIVE(births),\n    Drift = NAIVE(births ~ drift())\n  )\n\n# produce forecasts for period 2019-2022\npop_2019_22 &lt;- \n  pop_fit %&gt;% \n  forecast(new_data = pred_pop)\n\n# plot data with forecasts\npop_2019_22 %&gt;% \n  autoplot(ts_births %&gt;% filter(year &gt;=2014), level = NULL)+\n  autolayer(pred_pop, births, color = \"black\")\n\n\n\n\n\n\n\n\nFigure 12: Benchmark forecast methods\n\n\n\n\n\nFigure 12 demonstrates four methods of benchmark forecasting. The mean method forecasts all future values as the average of all historical values. The Naive method forecasts all future values as the last observed value. The naive-seasonal method forecasts each new value to be equal to the last observed value from the same season. The drift method allows changes to increase or decrease in time, where the gradient is set as the average change seen in the historical data. In this figure the last four years were forecasted using the four methods."
  },
  {
    "objectID": "posts/canada-births-time-series/index.html#exponential-smoothing",
    "href": "posts/canada-births-time-series/index.html#exponential-smoothing",
    "title": "Canadian Births Time Series Analysis",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nHistorically this technique has often been used for forecasting. Forecasts are produced by weighting past observations in an exponential manner. That is, the more recent the observation, the greater the weighting towards the forecast. A list of exponential smoothing factors are noted in chapter 8.4 of the fpp3 webbook.\n\nHolt-Winters method\nHolt-Winters method accounts not only for trend but also seasonality. The method comprises the forecasting equation, as well as three smoothing equations accounting for the level, trend and seasonality of the data.\nThe two variations of this method relate to the seasonal component. If the seasonal variations are constant though the series then the additive method is chosen. If however the variations are changing proportional to the level of the series then the multiplicative method is chosen.\nCode for the two seasonality models are shown below. A comparison of the fits for both models are compared.\n\n\n\nTable 1: Comparison of Additive and Multiplicative HW methods\n\n\n\nShow the code\nfit &lt;- \n  ts_births %&gt;% \n  model(\n    additive = ETS(births ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n    multiplicative = ETS(births ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n    )\n\nfc &lt;- fit %&gt;% forecast() \n\n\n\n\n\n\nFits of additive and multiplicative HW models\n\n\nShow the code\naugment(fit)\n\n\n# A tsibble: 768 x 6 [1M]\n# Key:       .model [2]\n   .model     ymdate births .fitted  .resid  .innov\n   &lt;chr&gt;       &lt;mth&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 additive 1991 Jan  32213  33606. -1393.  -1393. \n 2 additive 1991 Feb  30345  30936.  -591.   -591. \n 3 additive 1991 Mar  34869  33865.  1004.   1004. \n 4 additive 1991 Apr  35398  34303.  1095.   1095. \n 5 additive 1991 May  36371  36493.  -122.   -122. \n 6 additive 1991 Jun  34378  35478. -1100.  -1100. \n 7 additive 1991 Jul  35436  35931.  -495.   -495. \n 8 additive 1991 Aug  34421  34932.  -511.   -511. \n 9 additive 1991 Sep  34410  34465.   -54.5   -54.5\n10 additive 1991 Oct  33092  33223.  -131.   -131. \n# ℹ 758 more rows\n\n\nShow the code\ntidy(fit) %&gt;% \n  spread(.model, estimate)\n\n\n# A tibble: 17 × 3\n   term       additive multiplicative\n   &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1 alpha      0.761           0.401  \n 2 b[0]     -26.3           -43.4    \n 3 beta       0.000112        0.00838\n 4 gamma      0.00241         0.360  \n 5 l[0]   34964.          35324.     \n 6 s[-1]  -1850.              0.911  \n 7 s[-10] -2915.              0.918  \n 8 s[-11] -1331.              0.953  \n 9 s[-2]    173.              0.975  \n10 s[-3]   1347.              1.02   \n11 s[-4]   1399.              1.01   \n12 s[-5]   1994.              1.06   \n13 s[-6]    678.              1.03   \n14 s[-7]   1573.              1.09   \n15 s[-8]    190.              1.05   \n16 s[-9]    490.              1.04   \n17 s[0]   -1748.              0.944  \n\n\nNote that for the additive model, all the seasonal components add to 0. For the multiplicative model, all the seasonal components add to 1.\n\n\nShow the code\nfc %&gt;% \n  autoplot(filter(ts_births, ymdate &gt;= yearmonth(\"2018 Jan\")), level = 95)\n\n\n\n\n\n\n\n\nFigure 13: Comparison between the additive and multiplicative models\n\n\n\n\n\nIn Figure 13 the difference between additive and multiplicative levels is small. However, the 95% prediction intervals are noticably greater for the additive model.\nWe can also investigate the effect of dampening on the additive (or multiplicative) model. This is a common method that performs extremely well. See @11.40\n\n\nShow the code\nts_births %&gt;% \n  filter(year&lt;2019) %&gt;% \n  model(\n    hl = ETS(births ~ error(\"A\") + trend(\"Ad\") + season(\"A\"))\n  ) %&gt;% \n  forecast(h = \"48 months\") %&gt;% \nautoplot(ts_births |&gt; filter(between (ymdate, yearmonth(\"2018 Jan\"), yearmonth(\"2022 Dec\"))))+\n  labs(title = str_wrap(\"80% & 95% prediction intervals for the dampend additive model on Canadian population for 2019-2022.\",80),\n       subtitle = c(\"The black line represents actual population values. The blue line represents the mean forecast.\"),\n       x = \"Date\")\n\n\n\n\n\n\n\n\nFigure 14: Seasonal Additive Exponential Smoothing with Trend Dampening\n\n\n\n\n\nThe 80% and 95% confidence intervals calculated using the holt-linear method are displayed in Figure 14. The ETS function utilises state-space modelling to calculate the confidence intervals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew-blog",
    "section": "",
    "text": "Canadian Births Time Series Analysis\n\n\n\n\n\n\nTime-series\n\n\nModelling\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nAndrew Saul\n\n\n\n\n\n\n\n\n\n\n\n\nCanadian Births Time Series Analysis\n\n\n\n\n\n\nTime-series\n\n\nModelling\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nAndrew Saul\n\n\n\n\n\n\n\n\n\n\n\n\nR Patchwork hacks I found useful\n\n\n\n\n\n\nPatchwork\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nAndrew Saul\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Bayesian Simple Linear Regression with Stan R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 19, 2025\n\n\nAndrew Saul\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 13, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Bayesian simple linear regression/index.html",
    "href": "posts/Bayesian simple linear regression/index.html",
    "title": "Implementing Bayesian Simple Linear Regression with Stan R",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(GGally)\nlibrary(bayesplot)\n\n\n\nIntroduction\nThis is an instructional blog about creating a Markov Chain Monte Carlo (MCMC) linear regression model using Stan R. In order to keep things simple I decided to use only one explanatory variable in my model. The model was based on the relationship between height and weight of adults and data was generated using R software.\n\nSoftware preparation\nIt was essential that the Stan software package ran on my machine. Instructions how to do this are found here. Through experience I found it necessary to ensure the latest version of R, Rtools and RStudio was loaded. This required the packages on my system being removed before the latest versions were installed.\n\n\nReferences\nRichard Mcelreath has illuminated the field of Bayesian Statistics to many through his excellent book Statistical Rethinking and associated youtube presentations of his course.\nThe “Coding Club” give a good step by step guide to implementing MCMC linear regression here.\n\n\n\n\n\n\n\n\n\nInvestigating Relationships\nAccording to Mcelreath, before one implements linear regression, one should :\n\nState the question they are investigating. In the example provided, the question would be “What is the relationship between height and weight in the adult population?”. Therefore the parameters in the population (estimands) being estimated need to be defined. Here the estimands would be the y-intercept \\(\\alpha\\) and the gradient \\(\\beta\\) parameters describing the simple linear equation.\n, Propose a scientific model by sketching the causal assumptions ie. the dependencies between the variables.\nCheck that the scientific model produces realistic results. Any code may have bugs. By creating synthetic data where the outcomes are known the validity of the model can be checked before being implemented on real data.\n\nThis blog will describe the process of creating a model for simulated height and weight measurements of adults. in R.\n\n\n\nGenerative model\nWeight (W) in kg can be considered as a proportion of height(H) in cm as well as being influenced by unobserved causes(U). This is summarised in the equation (Equation 1)\n\\[\nW = \\beta*H - U\n\\tag{1}\\]\nGiven a vector of heights, the function to generate corresponding weights is as follows\n\n\nCode\nsim_weight &lt;- function(H,b,sd){\n  U &lt;- rnorm(length(H), 0, sd)\n  W &lt;- b*H +U\n}\n\n\nAccording to our generative model, heights are linearly proportional to weights, with some noise due to the unobserved variables. Using the values b=0.5 and sd = 5, weights are calculated according to the above formula\n\n\nCode\nset.seed(25)\nN &lt;- 20\nb &lt;- 0.5\nsd &lt;- 5\nH &lt;- runif(N, 130, 190)\nW &lt;- sim_weight(H, b, sd )\nHW_df &lt;- tibble(Height = H, Weight = W)\n\n\nThe resulting plot is seen in Figure 1\n\n\nCode\nggplot(HW_df, aes(x= H, y = W))+\n  geom_point()+\n  labs(x = \"Height (cm)\",\n       y = \"Weight (kg)\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 1: Plot of generated points using sim_weight function\n\n\n\n\n\n\nData Centring\nBayesian models require “Prior” distributions associated with each parameter. A prior acts as a regulariser to the observed data. We therefore need to have a ballpark idea as to observed distribution of each parameter. By centering the data on the mean height, the y-intercept term now represents the weight of a person of mean height.\n\n\nCode\nH_bar &lt;- mean(H)\ncentred_H &lt;- H-H_bar\ncentred_range &lt;- range(centred_H)\nHW_df &lt;- HW_df %&gt;% bind_cols(centred_H = centred_H)\n\n\n\n\nCode\nggplot(HW_df, aes(x= centred_H, y = W))+\n  geom_point()+\n  labs(x = paste0(\"Height (from\", round(H_bar, 0),\" cm)\"),\n       y = \"Weight (kg)\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 2: Data that has been centred.\n\n\n\n\n\nFor comparison, parameters obtained using the frequentist linear regression are displayed below.\n\n\nCode\nlm_fit &lt;- lm(W~centred_H, data = HW_df)\nsummary(lm_fit)\n\n\n\nCall:\nlm(formula = W ~ centred_H, data = HW_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9546 -4.9650 -0.9118  3.8723 12.9614 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 79.25285    1.30692  60.641  &lt; 2e-16 ***\ncentred_H    0.46496    0.07915   5.875 1.46e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.845 on 18 degrees of freedom\nMultiple R-squared:  0.6572,    Adjusted R-squared:  0.6382 \nF-statistic: 34.51 on 1 and 18 DF,  p-value: 1.459e-05\n\n\nFor the centred data, $= $ 79.3, $= $ 0.46 and $= $ 5.845\n\n\nPriors\nAs Figure 2 is centred, it is easier to estimate the prior distribution of \\(\\alpha\\). This parameter should have a normal distribution centred on the mean weight of an individual of 160 cm. Since we know the linear model of height vs weight has a positive gradient a lognormal distribution can be used for \\(\\beta\\). Mcelreath often uses an exponential model for the variance of the model. As we know that the standard deviation of simulations around the regression line is 5, we need to calculate the value of the exponential model parameter \\(\\lambda\\). We know that \\(\\lambda\\) is the inverse of its mean. On the conservative side, if we say the mean value of the variance is 10, then the value of \\(\\lambda\\) should be set at 0.1.\nA bayesian model for the generated data is defined below\n\\[\nW_i\\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta*(H_i - \\bar{H})  \\\\\n\\alpha \\sim Normal(75, 10) \\\\\n\\beta \\sim LogNormal(0,1) \\\\\n\\sigma \\sim Exp(0.1)\n\\]\nOne question Mcelreath asks for any model is the about the accuracy of the priors.\n\n\n\nPrior simulations\nin order to answer this question we need to perform prior prediction simulations. The code for \\(\\alpha\\) and \\(\\beta\\) simulations is displayed below.\n\n\nCode\nn &lt;- 1000\na &lt;- rnorm(n, 75, 15)\nb &lt;- rlnorm(n, 0, 1)\n\n\nA plot of the prior predictive simulations is displayed in Figure 3.\n\n\nCode\nplot(NULL, xlim = c(-35, 35), ylim=c(-0, 150),\n     xlab= paste0(\"Centered Height (cm, 0 = \", round(H_bar, 0),\" cm)\"), \n                  ylab = \"Weight (kg)\")+\nfor (j in 1:50) abline(a=a[j], b=b[j])\n\n\ninteger(0)\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThe prior simulation demonstrates that at the centered value 0 \\(\\alpha\\) is centred on around 75 kg and has a range of 40 to 90 kg. The \\(\\alpha\\) prior is not so restrictive as to remove data from the generative model, but also not so loose as to permit extraordinary weight values in the model. Regarding \\(\\beta\\) there are some relatively flat lines representing little change in weight with height. There are however some steep lines indicating unrealistic dramatic weight gain with height. The \\(\\beta\\) prior could be tightened further as it appears quite weak. However, in linear regression a weak prior will have little effect on the model with increasing number of data points. The prior in effect acts as a single data point. For the sake of this exercise I will keep the priors as is.\nA note that prior settings are far more critical in non linear regression.\n\nData\nData needs to be in list form for stan models.\n\n\nCode\nstan_data &lt;- list(\n  N = N,\n  x = centred_H,\n  y = W\n)\n\n\n\n\nModel\nThe Stan code for the model is displayed below and written to a file. Within the code includes the definitions of the data, the parameters and the model. Generated quantities represent simulated data utilising the model. For assistance in writing Stan code, an AI engine such as ChatGPT can be used.\n\n\nCode\nwrite(\"// Stan model with simulations for simple linear regression\n\ndata {\n  int&lt;lower=0&gt; N;          // Number of observations\n  vector[N] x;             // Predictor (height)\n  vector[N] y;             // Response (weight)\n\n}\n\nparameters {\n  real alpha;              // Intercept\n  real beta;               // Slope\n  real&lt;lower=0&gt; sigma;     // Standard deviation of the residuals\n}\n\nmodel {\n  // Priors\n  alpha ~ normal(75, 10);\n  beta ~ lognormal(0, 1);\n  sigma ~ exponential(0.1);    // Weakly informative prior for sigma\n\n  // Likelihood\n  y ~ normal( alpha + beta * x, sigma);\n}\n\n generated quantities {\n   vector[N] ysim;        // Predicted values\n\n   for (i in 1:N) {\n     ysim[i] = normal_rng(alpha + beta * x[i], sigma);\n   }    \n  }  // posterior distribution\",\n\n\"stan_model1.stan\")\n\n\nThe command utilised to create the model is “stan().\n\n\nCode\nstan_model &lt;- \"stan_model1.stan\"\n\nfit &lt;- stan(file = stan_model, data = stan_data, warmup = 500, iter = 1000, chains = 4, cores = 4, thin = 1, seed = 1234)\n\n\nhash mismatch so recompiling; make sure Stan code ends with a blank line\n\n\nSummary statistics for the parameters is displayed below\n\n\nCode\nsummary(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))$summary\n\n\n            mean     se_mean         sd       2.5%       25%        50%\nalpha 79.1851986 0.036431290 1.40768527 76.4950297 78.312676 79.1883853\nbeta   0.4646649 0.002138231 0.08374583  0.3007056  0.407904  0.4638345\nsigma  6.1943126 0.032015679 1.15256463  4.4403892  5.420969  6.0047642\n            75%     97.5%    n_eff      Rhat\nalpha 80.079350 82.012210 1493.008 1.0003438\nbeta   0.518671  0.629741 1533.972 0.9993472\nsigma  6.789685  8.940377 1296.000 1.0001971\n\n\nThe Rhat output for each parameter is close to 1. This indicates that the chains have converged.\n\n\nCode\ntraceplot(fit)\n\n\n'pars' not specified. Showing first 10 parameters by default.\n\n\n\n\n\n\n\n\n\nWith the overlapping of chains, the traceplots confirm the conclusion of the Rhat values ie. all chains converged. Note that this version of traceplot does not contain the warmup period.\n\n\nCode\njoint_post_table &lt;- rstan::extract(fit) \n\n\nThe joint posterior predictive distribution data is extracted utilising the “rstan::extract(fit)” code. The namespace stan:: must be used as there is a conflict with the extract function. In Figure 4 there are correlation plots between the three parameters of the model.\n\n\nCode\njoint_post_table[1:3] %&gt;% \n  pairs()\n\n\n\n\n\n\n\n\nFigure 4: Correlation plots between the parameters of the linear model\n\n\n\n\n\nBecause the data is centred, there should be no correlation between \\(\\alpha\\) and \\(\\beta\\) parameters. Additionally, no correlation should exist between \\(\\sigma\\) and the other parameters. The lack of correlations is displayed in Figure 4.\n\n\nCode\nma &lt;- mean(joint_post_table$alpha)\nmb &lt;- mean(joint_post_table$beta)\nmsig &lt;- mean(joint_post_table$sigma)\n\nxr &lt;- seq(-25,25,1)\nyCI &lt;- map(xr, ~quantile(joint_post_table$alpha +joint_post_table$beta * .x, probs = c(0.05, 0.95))) %&gt;% \n  bind_rows()\n\n\n\n\nCode\nxr &lt;- seq(-25,30,1)\nyCI &lt;- map(xr, ~quantile(joint_post_table$alpha +joint_post_table$beta * .x, probs = c(0.05, 0.95))) %&gt;% \n  bind_rows()\n\nggplot(stan_data %&gt;% as_tibble)+\n  geom_point(aes(x=x, y = y))+\n  geom_abline(aes(slope = beta, intercept = alpha), alpha=0.05,\n              data = joint_post_table[1:3] %&gt;% as_tibble())+\n  geom_abline(slope = mb, intercept = ma, color = \"red\", alpha = 0.5, linewidth = 1,\n              data = joint_post_table[1:3] %&gt;% as_tibble())+\n  geom_line(data = yCI, aes(x = xr, y = `5%`, color='red'), linewidth = 1)+\n  geom_line(data = yCI, aes(x = xr, y = `95%`, color='red'), linewidth = 1)+\n  theme_bw()+\n  labs(x = \"Centred Height (cm, 0 = 160 cm)\",\n       y = \"Weight (kg)\",\n       title = \"Distribution of 2000 mu samples with the mean and 90% credible interval\")\n\n\nWarning: `geom_abline()`: Ignoring `data` because `slope` and/or `intercept` were\nprovided.\n\n\n\n\n\n\n\n\nFigure 5: 2000 draws from the posterior distribution for alpha and beta, and the mean and 90% credible interval\n\n\n\n\n\nFigure 5 displays all the posterior distribution draws for \\(\\alpha\\) and \\(\\beta\\) in the form of lines. In addition Figure 5 demonstrates the 90% credible intervals of the posterior draws that form \\(\\mu\\). 90% was chosen due to computational stability and relation to Type-S errors see here\n\n\nCode\n#  5% & 95% credible intervals\nxr &lt;- seq(-25,25,1)\nyCI &lt;- map(xr, ~quantile(joint_post_table$alpha +joint_post_table$beta * .x, probs = c(0.05, 0.95))) %&gt;% \n  bind_rows()\n\nas_tibble(stan_data) %&gt;% \nggplot(aes(x=centred_H, y=W))+\n  geom_point()+\n  geom_abline(slope = mb, intercept = ma, color = \"black\")+\n  geom_line(data = yCI, aes(x = xr, y = `5%`, color='red'))+\n  geom_line(data = yCI, aes(x = xr, y = `95%`, color='red'))+\n  guides(color = \"none\")+\n  theme_bw()+\n  labs(x = \"Centred Height (cm, 0 = 160 cm)\",\n       title = \"5% & 95% Credible Intervals (red) and mean regression line \\nobtained from draws of alpha and beta posterior distributions\"\n       )\n\n\n\n\n\n\n\n\nFigure 6: 90% credible intervals for the posterior line draws.\n\n\n\n\n\n\n\nCode\npairs(joint_post_table[1:3])\n\n\n\n\n\n\n\n\nFigure 7: Pairs Plot. As data is centred, there should not be any correlation between alpha and beta.\n\n\n\n\n\nA pairs plot of the parameters is seen in Figure 7. One can see that no correlation exists between \\(\\alpha\\) and \\(\\beta\\). This is expected when the explanatory variable is centred.\n\n\nParameter summaries\n\n\nCode\npar(mfrow = c(1,3))\n\nplot(density(joint_post_table$alpha), main = \"Alpha\")\nabline(v =summary(lm_fit)$coefficients[1], col = 4, lty = 2)\n\nplot(density(joint_post_table$beta), main = \"Beta\")\nabline(v = summary(lm_fit)$coefficients[2], col = 4, lty = 2)\n\nplot(density(joint_post_table$sigma), main = \"Sigma\")\nabline(v = sigma(lm_fit), col = 4, lty = 2)\n\n\n\n\n\n\n\n\nFigure 8: The peak of each marginal distribution matches the Maximum Likelihood Estimation (MLE) from the frequentist model\n\n\n\n\n\nOne can see from Figure 8 that the mean marginal posterior distribution of each parameter matches its Maximum Likelihood Estimation (MLE) in the frequentist model.\nUseful functions to plot the marginal parameter distributions and histograms is stan_dens() and stan_hist() respectively\n\n\nCode\nstan_dens(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))\n\n\n\n\n\n\n\n\n\nCode\nstan_hist(fit, pars = c(\"alpha\", \"beta\", \"sigma\"))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn Figure 9, the mean marginal parameter estimates are displayed with their associated 90% credible intervals. It is difficult to see the credible intervals, especially for \\(\\beta\\) and \\(\\sigma\\), because the scale of the parameters is different. This plot is useful if all points are scaled.\n\n\nCode\nplot(fit, , pars = c(\"alpha\", \"beta\", \"sigma\"), show_density = FALSE, ci_level = 0.5, outer_level = 0.90, fill_color = \"salmon\")\n\n\nci_level: 0.5 (50% intervals)\n\n\nouter_level: 0.9 (90% intervals)\n\n\n\n\n\n\n\n\nFigure 9: Mean parameter estimates with their 90% Credible Intervals\n\n\n\n\n\n\n\nExtracting the y Predictions\n\n\nCode\ny_rep &lt;- as.matrix(fit, pars = \"ysim\")\ndim(y_rep)\n\n\n[1] 2000   20\n\n\nIn Figure 10, each light-blue line is a density plot created by simulating y values from a single draw of parameters from the posterior density distribution for each of the twenty x value data points. Five hundred iterations of simulations (light-blue density plots) are displayed, overlay-ed by the density plot for the y value datapoints (dark blue line). It is clear that the density plot of the observed values is contained well-within the range of density plots generated from the posterior distribution.\n\n\nCode\nppc_dens_overlay(W, y_rep[1:500, ])\n\n\n\n\n\n\n\n\nFigure 10: density distribution of simulated y values using parameter values drawn from the posterior denstiy distribution\n\n\n\n\n\nThe mean of each distribution can be plotted and compared to the mean of the observed values, as seen in Figure 11\n\n\nCode\nppc_stat(y = W, yrep = y_rep, stat = \"mean\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 11: Histogram of mean value for each simulated distribution (y_rep) compared to the mean value of the generated distribution (y)"
  },
  {
    "objectID": "posts/R Patchwork hacks off the beaten track/index.html",
    "href": "posts/R Patchwork hacks off the beaten track/index.html",
    "title": "R Patchwork hacks I found useful",
    "section": "",
    "text": "Introduction\nPresented are a few hacks using Patchwork that took me some time to discover.\nBelow is code that creates two categorical plots, each having the same ordered x and y axis labels. The task is to create a combined plot that uses only one set of y axis labels and legend. Both plots are to share the x-axis title and image caption.\n\nPlot Objects\n\n# creates a single plot depdendent on the \"cut\"  variable\nplot_func &lt;- function(var){\n  diamonds %&gt;% \n    filter(cut == var) %&gt;% \n    ggplot(aes(x = clarity, y = price, fill = color))+\n    geom_col(position = \"fill\")+\n    coord_flip()+\n    labs(title = var)\n}\n\n#saves the plots to a list object\nplots &lt;-   map(c(\"Ideal\", \"Fair\"), plot_func)\n\n\n\nDisplay\n\nDisplay the two plots side by side\n\nplots[[1]]+plots[[2]]\n\n\n\n\n\n\n\n\n\n\nUse a common legend\n\nplots[[1]]+plots[[2]] + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\nUse the same axes\n\nplots[[1]]+plots[[2]] + plot_layout(guides = \"collect\", axes = \"collect\")\n\n\n\n\n\n\n\n\n\n\nAnnotations\nUse “plot_annotations” function to give the patchwork plot a common annotation. Utilise the caption option in the plot_annotation function\n\n(plots[[1]]+plots[[2]]) + \n  plot_layout(guides = \"collect\", axes = \"collect\")+\n  plot_annotation(caption = \"This is the common caption for both plots\")\n\n\n\n\n\n\n\n\n\n\n\nChange name of x-axis title\nIf the title of the x-axis needs to be replaced, I found the above process didn’t work. Instead I removed the x-axis title from both plots with a NULL value in the plot function. At the bottom of both plots I printed the x-axis title. Please note that this centered x-axis title is below the normal level of the x-axis title.  The first action undertaken was to create a function where the x-axis title was converted to an empty string. Because the categories of the column geom were flipped with coord_flip, the x-axis features in the plots are infact y-axis features. As such the y labs string was given a NULL value.\n\n# creates a single plot depdendent on the \"cut\"  variable\nplot_func_xaxis &lt;- function(var){\n  diamonds %&gt;% \n    filter(cut == var) %&gt;% \n    ggplot(aes(x = clarity, y = price, fill = color))+\n    geom_col(position = \"fill\")+\n    coord_flip()+\n    labs(title = var,\n         y=NULL)\n}\n\nThe two plots were saved into a list object\n\nplot_change_xaxis &lt;-   map(c(\"Ideal\", \"Fair\"), plot_func_xaxis)\n\nThe plot_layout function was invoked so that only one legend and one set of y-axis variables were displayed.\n\nchange_xaxis_plots &lt;- \n(plot_change_xaxis[[1]]|plot_change_xaxis[[2]])+\n  plot_layout(guides = \"collect\", axes = \"collect\")\n\nFinally, in order to display a new x-axis title, the wrap_elements function was invoked and the tag attribute in the labs function was given the new x-axis title. By default, the tag element is displayed in the top_left corner of the plot. The plot.tag attributes invoked within the theme function can change the appearance of the tag string\n\nwrap_elements(panel = change_xaxis_plots)+\n  labs(tag = \"Proportion of total price\")+\n  theme(\n    plot.tag = element_text(size = rel(1.5)),\n    plot.tag.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nI hope the reader has found this code useful."
  }
]